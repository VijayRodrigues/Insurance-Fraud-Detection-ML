# ğŸš— Insurance Claims Fraud Detection  

## ğŸ“Œ Problem Statement  
Insurance fraud is a major challenge in the industry, leading to **billions of dollars in losses** every year. Fraudulent claims not only **increase operational costs** but also impact **honest policyholders**. Identifying fraud accurately is crucial for insurers to **minimize risks and optimize claim processing**.  

This project aims to develop a **Machine Learning model** that can predict **fraudulent insurance claims** based on **customer details, policy information, and accident reports**.  

---

## ğŸ¯ Objectives  
âœ”ï¸ Perform **Exploratory Data Analysis (EDA)** to understand fraud trends  
âœ”ï¸ Apply **Feature Engineering** to improve model accuracy  
âœ”ï¸ Train and evaluate **Machine Learning models** for fraud detection  
âœ”ï¸ Interpret results and generate insights for insurers  

---

## ğŸ“‚ Dataset  
We will be using an **auto insurance dataset** that contains:  
- **Customer Details** (Age, Gender, etc.)  
- **Policy Information** (Coverage, Vehicle Type, etc.)  
- **Accident Reports** (Cause, Severity, etc.)  
- **Claim Details** (Amount, Status: Fraud/Legit)  

ğŸ“¥ **Download Dataset**: [Automobile Insurance Fraud Data](https://github.com/dsrscientist/Data-Science-ML-Capstone-Projects/blob/master/Automobile_insurance_fraud.csv)  

---

## ğŸ› ï¸ Technologies Used  
- **Python** ğŸ  
- **Pandas & NumPy** ğŸ“Š â€“ Data Manipulation  
- **Matplotlib & Seaborn** ğŸ“ˆ â€“ Data Visualization  
- **Scikit-learn** ğŸ¤– â€“ Machine Learning  
- **XGBoost & Random Forest** ğŸŒ² â€“ Advanced ML models  

---

## ğŸ“Š Expected Outcomes  
âœ… Identify **patterns and key factors** in fraudulent claims  
âœ… Develop a **predictive model** to classify claims as **fraudulent or legitimate**  
âœ… Provide **data-driven insights** to help insurers reduce fraud  
âœ… Optimize **claim processing workflows** to improve efficiency  

---

## ğŸ“œ Project Structure  
```bash
Insurance-Fraud-Detection/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ insurance_fraud.csv      # Original dataset
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ cleaned_data.csv         # Processed and cleaned dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb                    # Exploratory Data Analysis
â”‚   â”œâ”€â”€ Model_Training.ipynb         # Model training and evaluation
â”‚   â””â”€â”€ Feature_Engineering.ipynb    # Feature selection and engineering
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ fraud_detection_model.pkl    # Trained Machine Learning model
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py        # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ model_training.py            # Model training and evaluation
â”‚   â”œâ”€â”€ utils.py                     # Utility functions
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ feature_importance.png       # Feature importance visualization
â”‚   â”œâ”€â”€ fraud_detection_report.txt   # Final model evaluation report
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_preprocessing.py   # Unit tests for data preprocessing
â”‚   â””â”€â”€ test_model_training.py       # Unit tests for model training
â”‚
â”œâ”€â”€ README.md                        # Project documentation
â”œâ”€â”€ requirements.txt                 # Dependencies
â”œâ”€â”€ LICENSE                          # License file
â””â”€â”€ .gitignore                       # Files to ignore in Git

```

## ğŸ” Model Selection & Evaluation  

To detect fraudulent insurance claims, I evaluated multiple machine learning models using **Receiver Operating Characteristic (ROC) curves** and **Area Under the Curve (AUC) scores** to compare their effectiveness. The models I tested include:  

âœ”ï¸ **Decision Tree Classifier**  
âœ”ï¸ **Random Forest Classifier**  
âœ”ï¸ **Gradient Boosting Classifier**  
âœ”ï¸ **AdaBoost Classifier**  
âœ”ï¸ **Extra Trees Classifier**  

### **ğŸ“Š Model Performance Insights**  

- **AUC Scores:** Higher AUC indicates better discrimination between fraudulent and genuine claims.  
- **Top Performers:** **Extra Trees Classifier** & **Random Forest Classifier** showed the best results with an AUC score of **0.95**, making them the most effective models for fraud detection.  
- **Lowest Performer:** **Decision Tree Classifier** had the lowest AUC (**0.79**), indicating weaker predictive power.  
- **Visual Comparison:** ROC curves provide a graphical representation of True Positive Rate (TPR) vs. False Positive Rate (FPR) for each model.  

### **ğŸ›  Model Implementation**  
I used **scikit-learn** to implement these models and evaluate their performance using:  

ğŸ”¹ `plot_roc_curve` â€“ For visualizing model comparisons.  
ğŸ”¹ `confusion_matrix`, `precision`, `recall`, `F1-score` â€“ For detailed performance metrics.  

By leveraging **ensemble models like Random Forest and Extra Trees**, I improved the accuracy and robustness of fraud detection.  
